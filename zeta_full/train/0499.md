<rating>Negative</rating>
<feedback>Add a comment explaining the statement.</feedback>
<events>
User edited "__main__.py":
```diff
@@ -17,7 +17,7 @@
 MAX_RETRIES = 3
 RETRY_DELAY = 5  # seconds
 
-
+# 
 tokenizer = transformers.AutoTokenizer.from_pretrained("Qwen/Qwen2-0.5B", trust_remote_code=True)
 
 def count_tokens(text: str) -> int:

```
</events>
<input>
```__main__.py
<|start_of_file|>
import transformers
import os
import requests
from typing import List, Dict
from tqdm import tqdm
import subprocess
import time
from requests.exceptions import RequestException

# Ollama API endpoint
OLLAMA_API_URL = "http://localhost:11434/api/generate"

<|editable_region_start|>
# Set the model name for Ollama
MODEL_NAME = "qwen2:0.5b" # or any other model you have in Ollama
OUTPUT_FILE = "summaries.xml"
MAX_TOKENS = 16000  # Adjust this based on the model's context window
MAX_RETRIES = 3
RETRY_DELAY = 5  # seconds

# <|user_cursor_is_here|>
tokenizer = transformers.AutoTokenizer.from_pretrained("Qwen/Qwen2-0.5B", trust_remote_code=True)

def count_tokens(text: str) -> int:
    return len(tokenizer.encode(text))

def split_text(text: str, max_tokens: int) -> List[str]:
    words = text.split()
    chunks = []
    for i in range(0, len(words), max_tokens):
        chunk = ' '.join(words[i:i + max_tokens])
        chunks.append(chunk)
    return chunks

def summarize_text(text: str, is_file: bool, is_truncated: bool = False) -> str:
    if is_file:
        prompt = f"Please summarize the following {'truncated ' if is_truncated else ''}file content in one paragraph, no more than 3 sentences total, and no bullet points or line breaks:\n\n{text}\n\nSummary:"
    else:
<|editable_region_end|>
        prompt = f"Please summarize the following {'truncated ' if is_truncated else ''}directory content overview in one paragraph, no more than 3 sentences total, and no bullet points or line breaks. Focus on the main themes and types of files/subdirectories present:\n\n{text}\n\nSummary:"
        print(prompt)

        try:
            response = requests.post(OLLAMA_API_URL, json={
                "model": MODEL_NAME,
                "prompt": prompt,
                "stream": False
            }, timeout=30)  # Set a timeout of 30 seconds

            if response.status_code == 200:
                json = response.json()
                print((json["eval_count"] / json["eval_duration"]) * 10**9)
                return json['response'].strip()
            elif response.status_code == 429:
                
            else:
                print(f"Error: {response.status_code}. Retrying in {RETRY_DELAY} seconds...")
        except RequestException as e:
            print(f"Request failed: {e}. Retrying in {RETRY_DELAY} seconds...")

        time.sleep(RETRY_DELAY)

    return "Error: Failed to get a response after multiple attempts."

def save_summary(file_path: str, summary: str):
    with open(OUTPUT_FILE, 'a', encoding='utf-8') as f:
        f.write(f"<path>{file_path}</path>\n<summary>{summary}</summary>\n\n")
    print(f"Summary appended to: {OUTPUT_FILE}")
```
</input>
<output>
```__main__.py
<|start_of_file|>
import transformers
import os
import requests
from typing import List, Dict
from tqdm import tqdm
import subprocess
import time
from requests.exceptions import RequestException

# Ollama API endpoint
OLLAMA_API_URL = "http://localhost:11434/api/generate"

<|editable_region_start|>
# Set the model name for Ollama
MODEL_NAME = "qwen2:0.5b" # or any other model you have in Ollama
OUTPUT_FILE = "summaries.xml"
MAX_TOKENS = 16000  # Adjust this based on the model's context window
MAX_RETRIES = 3
RETRY_DELAY = 5  # seconds

# Initialize tokenizer for text splitting and token counting
tokenizer = transformers.AutoTokenizer.from_pretrained("Qwen/Qwen2-0.5B", trust_remote_code=True)

def count_tokens(text: str) -> int:
    return len(tokenizer.encode(text))

def split_text(text: str, max_tokens: int) -> List[str]:
    words = text.split()
    chunks = []
    for i in range(0, len(words), max_tokens):
        chunk = ' '.join(words[i:i + max_tokens])
        chunks.append(chunk)
    return chunks

def summarize_text(text: str, is_file: bool, is_truncated: bool = False) -> str:
    if is_file:
        prompt = f"Please summarize the following {'truncated ' if is_truncated else ''}file content in one paragraph, no more than 3 sentences total, and no bullet points or line breaks:\n\n{text}\n\nSummary:"
    else:
<|editable_region_end|>
        prompt = f"Please summarize the following {'truncated ' if is_truncated else ''}directory content overview in one paragraph, no more than 3 sentences total, and no bullet points or line breaks. Focus on the main themes and types of files/subdirectories present:\n\n{text}\n\nSummary:"
        print(prompt)

        try:
            response = requests.post(OLLAMA_API_URL, json={
                "model": MODEL_NAME,
                "prompt": prompt,
                "stream": False
            }, timeout=30)  # Set a timeout of 30 seconds

            if response.status_code == 200:
                json = response.json()
                print((json["eval_count"] / json["eval_duration"]) * 10**9)
                return json['response'].strip()
            elif response.status_code == 429:
                
            else:
                print(f"Error: {response.status_code}. Retrying in {RETRY_DELAY} seconds...")
        except RequestException as e:
            print(f"Request failed: {e}. Retrying in {RETRY_DELAY} seconds...")

        time.sleep(RETRY_DELAY)

    return "Error: Failed to get a response after multiple attempts."

def save_summary(file_path: str, summary: str):
    with open(OUTPUT_FILE, 'a', encoding='utf-8') as f:
        f.write(f"<path>{file_path}</path>\n<summary>{summary}</summary>\n\n")
    print(f"Summary appended to: {OUTPUT_FILE}")
```
</output>
<outline>
```__main__.py
def count_tokens
def split_text
def summarize_text
def save_summary
def summarize_file
def get_git_tracked_files
def get_non_ignored_dirs
def summarize_directory
```

</outline>

<labels>
local-edit,infer-intent
</labels>
