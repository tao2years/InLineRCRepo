<rating>Negative</rating>
<feedback>Here I expect it to suggest and elif response.status_code == etc.</feedback>
<events>
User edited "__main__.py":
```diff
@@ -48,6 +48,7 @@
                 json = response.json()
                 print((json["eval_count"] / json["eval_duration"]) * 10**9)
                 return json['response'].strip()
+                
             else:
                 print(f"Error: {response.status_code}. Retrying in {RETRY_DELAY} seconds...")
         except RequestException as e:

```
</events>
<input>
```__main__.py
RETRY_DELAY = 5  # seconds

tokenizer = transformers.AutoTokenizer.from_pretrained("Qwen/Qwen2-0.5B", trust_remote_code=True)

def count_tokens(text: str) -> int:
    return len(tokenizer.encode(text))

def split_text(text: str, max_tokens: int) -> List[str]:
    words = text.split()
    chunks = []
    for i in range(0, len(words), max_tokens):
        chunk = ' '.join(words[i:i + max_tokens])
        chunks.append(chunk)
    return chunks

def summarize_text(text: str, is_file: bool, is_truncated: bool = False) -> str:
    if is_file:
        prompt = f"Please summarize the following {'truncated ' if is_truncated else ''}file content in one paragraph, no more than 3 sentences total, and no bullet points or line breaks:\n\n{text}\n\nSummary:"
    else:
        prompt = f"Please summarize the following {'truncated ' if is_truncated else ''}directory content overview in one paragraph, no more than 3 sentences total, and no bullet points or line breaks. Focus on the main themes and types of files/subdirectories present:\n\n{text}\n\nSummary:"
        print(prompt)

        try:
            response = requests.post(OLLAMA_API_URL, json={
                "model": MODEL_NAME,
                "prompt": prompt,
                "stream": False
            }, timeout=30)  # Set a timeout of 30 seconds

<|editable_region_start|>
            if response.status_code == 200:
                json = response.json()
                print((json["eval_count"] / json["eval_duration"]) * 10**9)
                return json['response'].strip()
                <|user_cursor_is_here|>
            else:
                print(f"Error: {response.status_code}. Retrying in {RETRY_DELAY} seconds...")
        except RequestException as e:
            print(f"Request failed: {e}. Retrying in {RETRY_DELAY} seconds...")

        time.sleep(RETRY_DELAY)

    return "Error: Failed to get a response after multiple attempts."

def save_summary(file_path: str, summary: str):
    with open(OUTPUT_FILE, 'a', encoding='utf-8') as f:
        f.write(f"<path>{file_path}</path>\n<summary>{summary}</summary>\n\n")
    print(f"Summary appended to: {OUTPUT_FILE}")

def summarize_file(file_path: str) -> str:
    print(f"Summarizing file: {file_path}")
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            content = file.read()

        is_truncated = False
        if count_tokens(content) > MAX_TOKENS:
            chunks = split_text(content, MAX_TOKENS)
            chunk_summaries = []
            for i, chunk in enumerate(chunks):
                print(f"  Processing chunk {i+1}/{len(chunks)} of {file_path}")
                chunk_summaries.append(summarize_text(chunk, is_file=True))
            summary = summarize_text("\n".join(chunk_summaries), is_file=True, is_truncated=True)
            is_truncated = True
        else:
            summary = summarize_text(content, is_file=True)

<|editable_region_end|>
        print(f"File summary for {file_path}:\n{summary}")
```
</input>
<output>
```__main__.py
RETRY_DELAY = 5  # seconds

tokenizer = transformers.AutoTokenizer.from_pretrained("Qwen/Qwen2-0.5B", trust_remote_code=True)

def count_tokens(text: str) -> int:
    return len(tokenizer.encode(text))

def split_text(text: str, max_tokens: int) -> List[str]:
    words = text.split()
    chunks = []
    for i in range(0, len(words), max_tokens):
        chunk = ' '.join(words[i:i + max_tokens])
        chunks.append(chunk)
    return chunks

def summarize_text(text: str, is_file: bool, is_truncated: bool = False) -> str:
    if is_file:
        prompt = f"Please summarize the following {'truncated ' if is_truncated else ''}file content in one paragraph, no more than 3 sentences total, and no bullet points or line breaks:\n\n{text}\n\nSummary:"
    else:
        prompt = f"Please summarize the following {'truncated ' if is_truncated else ''}directory content overview in one paragraph, no more than 3 sentences total, and no bullet points or line breaks. Focus on the main themes and types of files/subdirectories present:\n\n{text}\n\nSummary:"
        print(prompt)

        try:
            response = requests.post(OLLAMA_API_URL, json={
                "model": MODEL_NAME,
                "prompt": prompt,
                "stream": False
            }, timeout=30)  # Set a timeout of 30 seconds

<|editable_region_start|>
            if response.status_code == 200:
                json = response.json()
                print((json["eval_count"] / json["eval_duration"]) * 10**9)
                return json['response'].strip()
            elif response.status_code == 429:
                print(f"Rate limit exceeded")
            else:
                print(f"Error: {response.status_code}. Retrying in {RETRY_DELAY} seconds...")
        except RequestException as e:
            print(f"Request failed: {e}. Retrying in {RETRY_DELAY} seconds...")

        time.sleep(RETRY_DELAY)

    return "Error: Failed to get a response after multiple attempts."

def save_summary(file_path: str, summary: str):
    with open(OUTPUT_FILE, 'a', encoding='utf-8') as f:
        f.write(f"<path>{file_path}</path>\n<summary>{summary}</summary>\n\n")
    print(f"Summary appended to: {OUTPUT_FILE}")

def summarize_file(file_path: str) -> str:
    print(f"Summarizing file: {file_path}")
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            content = file.read()

        is_truncated = False
        if count_tokens(content) > MAX_TOKENS:
            chunks = split_text(content, MAX_TOKENS)
            chunk_summaries = []
            for i, chunk in enumerate(chunks):
                print(f"  Processing chunk {i+1}/{len(chunks)} of {file_path}")
                chunk_summaries.append(summarize_text(chunk, is_file=True))
            summary = summarize_text("\n".join(chunk_summaries), is_file=True, is_truncated=True)
            is_truncated = True
        else:
            summary = summarize_text(content, is_file=True)

<|editable_region_end|>
        print(f"File summary for {file_path}:\n{summary}")
```
</output>
<outline>
```__main__.py
def count_tokens
def split_text
def summarize_text
def save_summary
def summarize_file
def get_git_tracked_files
def get_non_ignored_dirs
def summarize_directory
```

</outline>

<labels>
local-edit,complete-pattern
</labels>
