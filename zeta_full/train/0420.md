<rating>Negative</rating>
<feedback>I just deleted this and the method does not exist.</feedback>
<events>
User edited "crates/collab/src/llm.rs":
```diff
@@ -470,7 +470,7 @@
         .replace("<outline>", &outline_prefix)
         .replace("<events>", &params.input_events)
         .replace("<excerpt>", &params.input_excerpt);
-    let mut response = fireworks::complete_text(
+    let mut response = fireworks::(
         &state.http_client,
         api_url,
         api_key,

```
</events>
<input>
```crates/collab/src/llm.rs
    _country_code_header: Option<TypedHeader<CloudflareIpCountryHeader>>,
    Json(params): Json<PredictEditsParams>,
) -> Result<impl IntoResponse> {
    if !claims.is_staff {
        return Err(anyhow!("not found"))?;
    }

    let api_url = state
        .config
        .prediction_api_url
        .as_ref()
        .context("no PREDICTION_API_URL configured on the server")?;
    let api_key = state
        .config
        .prediction_api_key
        .as_ref()
        .context("no PREDICTION_API_KEY configured on the server")?;
    let model = state
        .config
        .prediction_model
        .as_ref()
        .context("no PREDICTION_MODEL configured on the server")?;

    let outline_prefix = params
        .outline
        .as_ref()
        .map(|outline| format!("### Outline for current file:\n{}\n", outline))
        .unwrap_or_default();

    let prompt = include_str!("./llm/prediction_prompt.md")
        .replace("<outline>", &outline_prefix)
<|editable_region_start|>
        .replace("<events>", &params.input_events)
        .replace("<excerpt>", &params.input_excerpt);
    let mut response = fireworks::<|user_cursor_is_here|>(
        &state.http_client,
<|editable_region_end|>
        api_url,
        api_key,
        fireworks::CompletionRequest {
            model: model.to_string(),
            prompt: prompt.clone(),
            max_tokens: 2048,
            temperature: 0.,
            prediction: Some(fireworks::Prediction::Content {
                content: params.input_excerpt,
            }),
            rewrite_speculation: Some(true),
        },
    )
    .await?;
    let choice = response
        .choices
        .pop()
        .context("no output from completion response")?;
    Ok(Json(PredictEditsResponse {
        output_excerpt: choice.text,
    }))
}

/// The maximum monthly spending an individual user can reach on the free tier
/// before they have to pay.
pub const FREE_TIER_MONTHLY_SPENDING_LIMIT: Cents = Cents::from_dollars(10);

/// The default value to use for maximum spend per month if the user did not
/// explicitly set a maximum spend.
///
/// Used to prevent surprise bills.
pub const DEFAULT_MAX_MONTHLY_SPEND: Cents = Cents::from_dollars(10);
```
</input>
<output>
```crates/collab/src/llm.rs
    _country_code_header: Option<TypedHeader<CloudflareIpCountryHeader>>,
    Json(params): Json<PredictEditsParams>,
) -> Result<impl IntoResponse> {
    if !claims.is_staff {
        return Err(anyhow!("not found"))?;
    }

    let api_url = state
        .config
        .prediction_api_url
        .as_ref()
        .context("no PREDICTION_API_URL configured on the server")?;
    let api_key = state
        .config
        .prediction_api_key
        .as_ref()
        .context("no PREDICTION_API_KEY configured on the server")?;
    let model = state
        .config
        .prediction_model
        .as_ref()
        .context("no PREDICTION_MODEL configured on the server")?;

    let outline_prefix = params
        .outline
        .as_ref()
        .map(|outline| format!("### Outline for current file:\n{}\n", outline))
        .unwrap_or_default();

    let prompt = include_str!("./llm/prediction_prompt.md")
        .replace("<outline>", &outline_prefix)
<|editable_region_start|>
        .replace("<events>", &params.input_events)
        .replace("<excerpt>", &params.input_excerpt);
    let mut response = fireworks::(
        &state.http_client,
<|editable_region_end|>
        api_url,
        api_key,
        fireworks::CompletionRequest {
            model: model.to_string(),
            prompt: prompt.clone(),
            max_tokens: 2048,
            temperature: 0.,
            prediction: Some(fireworks::Prediction::Content {
                content: params.input_excerpt,
            }),
            rewrite_speculation: Some(true),
        },
    )
    .await?;
    let choice = response
        .choices
        .pop()
        .context("no output from completion response")?;
    Ok(Json(PredictEditsResponse {
        output_excerpt: choice.text,
    }))
}

/// The maximum monthly spending an individual user can reach on the free tier
/// before they have to pay.
pub const FREE_TIER_MONTHLY_SPENDING_LIMIT: Cents = Cents::from_dollars(10);

/// The default value to use for maximum spend per month if the user did not
/// explicitly set a maximum spend.
///
/// Used to prevent surprise bills.
pub const DEFAULT_MAX_MONTHLY_SPEND: Cents = Cents::from_dollars(10);
```
</output>
<outline>
```crates/collab/src/llm.rs
mod authorization
pub mod db
mod token
pub struct LlmState
 pub config
 pub executor
 pub db
 pub http_client
 pub kinesis_client
 active_user_count_by_model
const ACTIVE_USER_COUNT_CACHE_DURATION
impl LlmState
 pub async fn new
 pub async fn get_active_user_count
pub fn routes
async fn validate_api_token
async fn list_models
async fn perform_completion
fn normalize_model_name
async fn predict_edits
pub const FREE_TIER_MONTHLY_SPENDING_LIMIT
pub const DEFAULT_MAX_MONTHLY_SPEND
async fn check_usage_limit
struct CompletionChunk
 bytes
 input_tokens
 output_tokens
 cache_creation_input_tokens
 cache_read_input_tokens
struct TokenCountingStream
 state
 claims
 provider
 model
 tokens
 inner_stream
impl Stream for TokenCountingStream<S>
 type Item
 fn poll_next
impl Drop for TokenCountingStream<S>
 fn drop
pub fn log_usage_periodically
```

</outline>

<labels>
no-op,infer-intent
</labels>
