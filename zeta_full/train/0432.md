<rating>Positive</rating>
<feedback>Very good</feedback>
<events>
User edited "crates/collab/src/llm.rs":
```diff
@@ -488,6 +488,7 @@
         },
     )
     .await?;
+    let duration = start.elapsed();
 
     state.executor.spawn_detached({
         let kinesis_client = state.kinesis_client.clone();

```

User edited "crates/collab/src/llm.rs":
```diff
@@ -505,6 +505,7 @@
                 json!({
                     "model": model.to_string(),
                     "headers": headers,
+                    "
                 }),
             )
             .write(&kinesis_client, &kinesis_stream)

```
</events>
<input>
```crates/collab/src/llm.rs
    let mut response = fireworks::complete(
        &state.http_client,
        api_url,
        api_key,
        fireworks::CompletionRequest {
            model: model.to_string(),
            prompt: prompt.clone(),
            max_tokens: 2048,
            temperature: 0.,
            prediction: Some(fireworks::Prediction::Content {
                content: params.input_excerpt,
            }),
            rewrite_speculation: Some(true),
        },
    )
    .await?;
    let duration = start.elapsed();

    state.executor.spawn_detached({
        let kinesis_client = state.kinesis_client.clone();
        let kinesis_stream = state.config.kinesis_stream.clone();
        let headers = response.headers.clone();
        let model = model.clone();

<|editable_region_start|>
        async move {
            SnowflakeRow::new(
                "Fireworks Completion Requested",
                claims.metrics_id,
                claims.is_staff,
                claims.system_id.clone(),
                json!({
                    "model": model.to_string(),
                    "headers": headers,
                    "<|user_cursor_is_here|>
                }),
            )
            .write(&kinesis_client, &kinesis_stream)
            .await
            .log_err();
        }
    });

    let choice = response
        .completion
        .choices
        .pop()
        .context("no output from completion response")?;
    Ok(Json(PredictEditsResponse {
        output_excerpt: choice.text,
<|editable_region_end|>
    }))
}

/// The maximum monthly spending an individual user can reach on the free tier
/// before they have to pay.
pub const FREE_TIER_MONTHLY_SPENDING_LIMIT: Cents = Cents::from_dollars(10);

/// The default value to use for maximum spend per month if the user did not
/// explicitly set a maximum spend.
///
/// Used to prevent surprise bills.
pub const DEFAULT_MAX_MONTHLY_SPEND: Cents = Cents::from_dollars(10);

async fn check_usage_limit(
    state: &Arc<LlmState>,
    provider: LanguageModelProvider,
    model_name: &str,
    claims: &LlmTokenClaims,
```
</input>
<output>
```crates/collab/src/llm.rs
    let mut response = fireworks::complete(
        &state.http_client,
        api_url,
        api_key,
        fireworks::CompletionRequest {
            model: model.to_string(),
            prompt: prompt.clone(),
            max_tokens: 2048,
            temperature: 0.,
            prediction: Some(fireworks::Prediction::Content {
                content: params.input_excerpt,
            }),
            rewrite_speculation: Some(true),
        },
    )
    .await?;
    let duration = start.elapsed();

    state.executor.spawn_detached({
        let kinesis_client = state.kinesis_client.clone();
        let kinesis_stream = state.config.kinesis_stream.clone();
        let headers = response.headers.clone();
        let model = model.clone();

<|editable_region_start|>
        async move {
            SnowflakeRow::new(
                "Fireworks Completion Requested",
                claims.metrics_id,
                claims.is_staff,
                claims.system_id.clone(),
                json!({
                    "model": model.to_string(),
                    "headers": headers,
                    "duration": duration.as_millis()
                }),
            )
            .write(&kinesis_client, &kinesis_stream)
            .await
            .log_err();
        }
    });

    let choice = response
        .completion
        .choices
        .pop()
        .context("no output from completion response")?;
    Ok(Json(PredictEditsResponse {
        output_excerpt: choice.text,
<|editable_region_end|>
    }))
}

/// The maximum monthly spending an individual user can reach on the free tier
/// before they have to pay.
pub const FREE_TIER_MONTHLY_SPENDING_LIMIT: Cents = Cents::from_dollars(10);

/// The default value to use for maximum spend per month if the user did not
/// explicitly set a maximum spend.
///
/// Used to prevent surprise bills.
pub const DEFAULT_MAX_MONTHLY_SPEND: Cents = Cents::from_dollars(10);

async fn check_usage_limit(
    state: &Arc<LlmState>,
    provider: LanguageModelProvider,
    model_name: &str,
    claims: &LlmTokenClaims,
```
</output>
<outline>
```crates/collab/src/llm.rs
mod authorization
pub mod db
mod token
pub struct LlmState
 pub config
 pub executor
 pub db
 pub http_client
 pub kinesis_client
 active_user_count_by_model
const ACTIVE_USER_COUNT_CACHE_DURATION
impl LlmState
 pub async fn new
 pub async fn get_active_user_count
pub fn routes
async fn validate_api_token
async fn list_models
async fn perform_completion
fn normalize_model_name
const FREE_TIER_MONTHLY_SPENDING_LIMIT
pub const DEFAULT_MAX_MONTHLY_SPEND
async fn check_usage_limit
struct CompletionChunk
 bytes
 input_tokens
 output_tokens
 cache_creation_input_tokens
 cache_read_input_tokens
struct TokenCountingStream
 state
 claims
 provider
 model
 tokens
 inner_stream
impl Stream for TokenCountingStream<S>
 type Item
 fn poll_next
impl Drop for TokenCountingStream<S>
 fn drop
```

</outline>

<labels>
local-edit,infer-intent
</labels>
