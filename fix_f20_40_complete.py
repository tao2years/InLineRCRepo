#!/usr/bin/env python3
"""
å®Œå…¨é‡æ–°æ„é€ F20-40 benchmarkæ–‡ä»¶
ä»gpt5_results_20-40ç›®å½•å’ŒåŸå§‹benchmarkæ•°æ®æ„é€ å®Œæ•´çš„separatedæ ¼å¼
"""

import json
import os
import re
from typing import Dict, List, Any

def load_original_benchmark_data():
    """åŠ è½½åŸå§‹benchmarkæ•°æ®"""
    original_data = {}
    
    # åŠ è½½åŸå§‹F20-40æ•°æ®
    with open('benchmark/nl2code_F20-40.jsonl', 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                data = json.loads(line)
                original_data[data['id']] = data
    
    print(f"åŠ è½½äº† {len(original_data)} ä¸ªåŸå§‹æ¡ç›®")
    return original_data

def parse_gpt5_file(file_path: str) -> Dict[str, Any]:
    """è§£æGPT-5ç»“æœæ–‡ä»¶"""
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # æå–æ–‡ä»¶ID
    filename = os.path.basename(file_path)
    file_id = filename.replace('.txt', '')
    
    # è§£æhunks_3, hunks_2, hunks_1
    hunks = {}
    for hunk_name in ['hunks_3', 'hunks_2', 'hunks_1']:
        # GPT-5æ–‡ä»¶ä¸­ä½¿ç”¨ hunks\_3 æ ¼å¼ï¼ˆè½¬ä¹‰ä¸‹åˆ’çº¿ï¼‰
        hunk_number = hunk_name.split('_')[1]  # æå–æ•°å­—éƒ¨åˆ†

        # ä½¿ç”¨æ›´ç®€å•çš„æ–¹æ³•ï¼šå…ˆæ‰¾åˆ°sectionï¼Œå†æå–JSON
        section_patterns = [
            f'### hunks\\_{hunk_number}',  # hunks\_3 æ ¼å¼
            f'### {hunk_name}',           # hunks_3 æ ¼å¼
        ]

        found = False
        for section_pattern in section_patterns:
            # æ‰¾åˆ°sectionçš„å¼€å§‹ä½ç½®
            section_start = content.find(section_pattern)
            if section_start != -1:
                # ä»sectionå¼€å§‹ä½ç½®æŸ¥æ‰¾JSONå—
                json_start = content.find('```json', section_start)
                if json_start != -1:
                    json_start += 7  # è·³è¿‡ ```json
                    json_end = content.find('```', json_start)
                    if json_end != -1:
                        json_content = content[json_start:json_end].strip()
                        try:
                            hunks[hunk_name] = json.loads(json_content)
                            found = True
                            print(f"âœ… æˆåŠŸè§£æ {hunk_name}: {len(hunks[hunk_name])} ä¸ªhunks")
                            break
                        except json.JSONDecodeError as e:
                            print(f"JSONè§£æé”™è¯¯ {hunk_name}: {e}")
                            continue

        if not found:
            print(f"è­¦å‘Š: åœ¨ {filename} ä¸­æœªæ‰¾åˆ° {hunk_name}")
            hunks[hunk_name] = []
    
    return {
        'id': file_id,
        'hunks': hunks
    }

def add_line_numbers_to_code(code_content: str, start_line: int = 1) -> str:
    """ä¸ºä»£ç æ·»åŠ è¡Œå·"""
    lines = code_content.split('\n')
    numbered_lines = []
    
    for i, line in enumerate(lines):
        line_num = start_line + i
        numbered_lines.append(f"{line_num:3d}: {line}")
    
    return '\n'.join(numbered_lines)

def normalize_code_content(content: str) -> str:
    """æ ‡å‡†åŒ–ä»£ç å†…å®¹ç”¨äºåŒ¹é…ï¼Œä½†ä¿ç•™ç¼©è¿›ç»“æ„"""
    if not content:
        return ""

    # ä¿ç•™ç¼©è¿›ï¼Œåªæ ‡å‡†åŒ–è¡Œå†…ç©ºæ ¼
    lines = content.split('\n')
    normalized_lines = []
    for line in lines:
        # ä¿ç•™è¡Œé¦–ç¼©è¿›ï¼Œæ ‡å‡†åŒ–è¡Œå†…ç©ºæ ¼
        stripped = line.strip()
        if stripped:
            # æ ‡å‡†åŒ–å¼•å·å’Œè¡Œå°¾æ ‡ç‚¹
            normalized = stripped.replace('\\"', '"').replace("\\'", "'")
            normalized = re.sub(r'[;,]\s*$', '', normalized)
            normalized_lines.append(normalized.lower())

    return '\n'.join(normalized_lines)

def preserve_original_indentation(content: str, original_content: str) -> str:
    """ä¿ç•™åŸå§‹ä»£ç çš„ç¼©è¿›æ ¼å¼"""
    if not content or not original_content:
        return content

    # å¦‚æœå†…å®¹åŒ¹é…ï¼Œè¿”å›åŸå§‹æ ¼å¼
    content_normalized = normalize_code_content(content)
    original_normalized = normalize_code_content(original_content)

    if content_normalized in original_normalized or original_normalized in content_normalized:
        return original_content.strip()

    return content

def find_best_match_in_context(target_content: str, context_lines: List[str]) -> tuple:
    """åœ¨contextä¸­æ‰¾åˆ°æœ€ä½³åŒ¹é…çš„è¡Œå·å’ŒåŸå§‹å†…å®¹"""
    if not target_content.strip():
        return -1, target_content

    target_normalized = normalize_code_content(target_content)
    if not target_normalized:
        return -1, target_content

    best_match_line = -1
    best_score = 0
    best_original_content = target_content

    for i, context_line in enumerate(context_lines):
        # æå–è¡Œå·å’Œå†…å®¹
        if ':' in context_line and context_line.strip():
            try:
                line_num_str = context_line.split(':', 1)[0].strip()
                if line_num_str.isdigit():
                    line_content = context_line.split(':', 1)[1]  # ä¿ç•™åŸå§‹ç¼©è¿›
                    line_normalized = normalize_code_content(line_content)

                    if not line_normalized:
                        continue

                    # è®¡ç®—ç›¸ä¼¼åº¦
                    if target_normalized == line_normalized:
                        # å®Œå…¨åŒ¹é…ï¼Œè¿”å›åŸå§‹æ ¼å¼çš„å†…å®¹
                        return int(line_num_str), line_content.strip()

                    # æ£€æŸ¥åŒ…å«å…³ç³»
                    if target_normalized in line_normalized or line_normalized in target_normalized:
                        score = 0.9
                        if score > best_score:
                            best_score = score
                            best_match_line = int(line_num_str)
                            best_original_content = line_content.strip()

                    # å…³é”®è¯åŒ¹é…
                    target_words = set(re.findall(r'\w+', target_normalized))
                    line_words = set(re.findall(r'\w+', line_normalized))

                    if target_words and line_words:
                        overlap = len(target_words & line_words)
                        total = len(target_words | line_words)
                        if total > 0:
                            score = overlap / total
                            # æé«˜é˜ˆå€¼åˆ°0.8ï¼Œç¡®ä¿æ›´å‡†ç¡®çš„åŒ¹é…
                            if score >= 0.8 and score > best_score:
                                best_score = score
                                best_match_line = int(line_num_str)
                                best_original_content = line_content.strip()
            except (ValueError, IndexError):
                continue

    if best_score >= 0.8:
        return best_match_line, best_original_content
    else:
        return -1, target_content

def parse_unified_diff_header(header_line: str) -> tuple:
    """è§£æunified diffçš„@@å¤´ï¼Œè¿”å›(old_start, old_count, new_start, new_count)"""
    # æ ¼å¼: @@ -old_start,old_count +new_start,new_count @@
    match = re.match(r'@@ -(\d+)(?:,(\d+))? \+(\d+)(?:,(\d+))? @@', header_line)
    if match:
        old_start = int(match.group(1))
        old_count = int(match.group(2)) if match.group(2) else 1
        new_start = int(match.group(3))
        new_count = int(match.group(4)) if match.group(4) else 1
        return old_start, old_count, new_start, new_count
    return None, None, None, None

def format_diff_with_line_numbers(diff_content: str, full_context: str) -> str:
    """ä¸ºdiffå†…å®¹æ·»åŠ æ­£ç¡®çš„è¡Œå·æ ‡æ³¨ï¼Œä¿ç•™åŸå§‹ç¼©è¿›"""
    if not diff_content.strip():
        return diff_content

    lines = diff_content.split('\n')
    formatted_lines = []
    context_lines = full_context.split('\n')

    # å½“å‰å¤„ç†çŠ¶æ€
    old_line_num = 1
    new_line_num = 1

    for line in lines:
        if line.startswith('@@'):
            # è§£ædiffå¤´ï¼Œè·å–èµ·å§‹è¡Œå·
            old_start, old_count, new_start, new_count = parse_unified_diff_header(line)
            if old_start is not None:
                old_line_num = old_start
                new_line_num = new_start
            formatted_lines.append(line)

        elif line.startswith('+') and not line.startswith('+++'):
            # å¤„ç†æ–°å¢è¡Œ
            content = line[1:]  # ä¿ç•™åŸå§‹ç©ºæ ¼
            if content.strip():  # åªæ£€æŸ¥æ˜¯å¦ä¸ºç©ºè¡Œ
                # åœ¨contextä¸­æŸ¥æ‰¾è¿™è¡Œä»£ç çš„çœŸå®ä½ç½®å’ŒåŸå§‹æ ¼å¼
                real_line_num, original_content = find_best_match_in_context(content, context_lines)
                if real_line_num > 0:
                    formatted_lines.append(f"+ {real_line_num:2d}: {original_content}")
                else:
                    # å¦‚æœæ‰¾ä¸åˆ°ï¼Œä½¿ç”¨é¢„æœŸçš„æ–°è¡Œå·å’ŒåŸå§‹å†…å®¹
                    formatted_lines.append(f"+ {new_line_num:2d}: {content.strip()}")
                new_line_num += 1
            else:
                formatted_lines.append(line)

        elif line.startswith('-') and not line.startswith('---'):
            # å¤„ç†åˆ é™¤è¡Œ
            content = line[1:]  # ä¿ç•™åŸå§‹ç©ºæ ¼
            if content.strip():  # åªæ£€æŸ¥æ˜¯å¦ä¸ºç©ºè¡Œ
                # åˆ é™¤çš„è¡Œä½¿ç”¨åŸå§‹è¡Œå·å’Œå†…å®¹
                formatted_lines.append(f"- {old_line_num:2d}: {content.strip()}")
                old_line_num += 1
            else:
                formatted_lines.append(line)

        elif line.startswith(' '):
            # ä¸Šä¸‹æ–‡è¡Œï¼ˆæœªå˜æ›´çš„è¡Œï¼‰
            content = line[1:]  # ä¿ç•™åŸå§‹ç©ºæ ¼
            if content.strip():  # åªæ£€æŸ¥æ˜¯å¦ä¸ºç©ºè¡Œ
                # åœ¨contextä¸­æŸ¥æ‰¾çœŸå®è¡Œå·å’ŒåŸå§‹æ ¼å¼
                real_line_num, original_content = find_best_match_in_context(content, context_lines)
                if real_line_num > 0:
                    formatted_lines.append(f"  {real_line_num:2d}: {original_content}")
                else:
                    # ä½¿ç”¨å½“å‰è¡Œå·å’ŒåŸå§‹å†…å®¹
                    formatted_lines.append(f"  {old_line_num:2d}: {content.strip()}")
                old_line_num += 1
                new_line_num += 1
            else:
                formatted_lines.append(line)
        else:
            # å…¶ä»–è¡Œï¼ˆå¦‚ç©ºè¡Œï¼‰ï¼Œä¿æŒä¸å˜
            formatted_lines.append(line)

    return '\n'.join(formatted_lines)

def format_recent_changes(hunks: Dict[str, List], full_context: str) -> str:
    """æ ¼å¼åŒ–Recent Changes"""
    rc_parts = []

    # RC3 (Earliest preparation work)
    if hunks.get('hunks_3'):
        rc_parts.append("### Recent Change 3 (Earliest preparation work)")
        for hunk in hunks['hunks_3']:
            rc_parts.append("```diff")
            diff_content = hunk.get('diff_content', '')
            formatted_diff = format_diff_with_line_numbers(diff_content, full_context)
            rc_parts.append(formatted_diff)
            rc_parts.append("```")
        rc_parts.append("")

    # RC2 (Intermediate preparation)
    if hunks.get('hunks_2'):
        rc_parts.append("### Recent Change 2 (Intermediate preparation)")
        for hunk in hunks['hunks_2']:
            rc_parts.append("```diff")
            diff_content = hunk.get('diff_content', '')
            formatted_diff = format_diff_with_line_numbers(diff_content, full_context)
            rc_parts.append(formatted_diff)
            rc_parts.append("```")
        rc_parts.append("")

    # RC1 (Latest preparation work)
    if hunks.get('hunks_1'):
        rc_parts.append("### Recent Change 1 (Latest preparation work)")
        for hunk in hunks['hunks_1']:
            rc_parts.append("```diff")
            diff_content = hunk.get('diff_content', '')
            formatted_diff = format_diff_with_line_numbers(diff_content, full_context)
            rc_parts.append(formatted_diff)
            rc_parts.append("```")
        rc_parts.append("")

    return '\n'.join(rc_parts)

def extract_context_from_prompt(prompt: str) -> Dict[str, str]:
    """ä»åŸå§‹promptä¸­æå–context aboveå’Œcontext below"""
    # æå–context above
    above_match = re.search(r'The context above is:\s*```java\s*(.*?)\s*```', prompt, re.DOTALL)
    context_above = above_match.group(1) if above_match else ""
    
    # æå–context below
    below_match = re.search(r'The context below is:\s*```java\s*(.*?)\s*```', prompt, re.DOTALL)
    context_below = below_match.group(1) if below_match else ""
    
    # æå–external classes
    external_match = re.search(r'Below are some information from external classes imported by current file:\s*```java\s*(.*?)\s*```', prompt, re.DOTALL)
    external_classes = external_match.group(1) if external_match else ""
    
    return {
        'context_above': context_above,
        'context_below': context_below,
        'external_classes': external_classes
    }

def calculate_line_numbers(context_above: str, good_example: str) -> Dict[str, int]:
    """è®¡ç®—è¡Œå·åˆ†é…"""
    above_lines = len(context_above.split('\n')) if context_above.strip() else 0
    good_lines = len(good_example.split('\n')) if good_example.strip() else 0
    
    # context aboveä»1å¼€å§‹
    above_start = 1
    
    # good exampleç´§æ¥ç€context above
    good_start = above_start + above_lines
    
    # context belowåœ¨good exampleä¹‹åï¼Œç•™ä¸€äº›é—´éš”
    below_start = good_start + good_lines + 2
    
    return {
        'above_start': above_start,
        'good_start': good_start,
        'below_start': below_start
    }

def create_complete_prompt(original_data: Dict, hunks: Dict[str, List]) -> str:
    """åˆ›å»ºå®Œæ•´çš„prompt"""
    # æå–åŸå§‹æ•°æ®
    contexts = extract_context_from_prompt(original_data['prompt'])
    good_example = original_data['good_example_response']
    query = original_data['extra_content']['query']

    # è®¡ç®—è¡Œå·
    line_nums = calculate_line_numbers(contexts['context_above'], good_example)

    # ä¸ºä»£ç æ·»åŠ è¡Œå·
    context_above_numbered = add_line_numbers_to_code(contexts['context_above'], line_nums['above_start'])
    context_below_numbered = add_line_numbers_to_code(contexts['context_below'], line_nums['below_start'])

    # æ„å»ºå®Œæ•´çš„contextç”¨äºè¡Œå·åŒ¹é…
    full_context = context_above_numbered + '\n' + context_below_numbered

    # æ ¼å¼åŒ–Recent Changesï¼ˆä¼ å…¥å®Œæ•´contextç”¨äºè¡Œå·åŒ¹é…ï¼‰
    recent_changes = format_recent_changes(hunks, full_context)

    # æ„å»ºå®Œæ•´prompt
    prompt = f"""A user is developing a new feature. Based on the known code information, help him implement this new feature.

Below are some information from external classes imported by current file:
```java
{contexts['external_classes']}
```

The context above is:
```java
{context_above_numbered}
```

The context below is:
```java
{context_below_numbered}
```

## Recent Changes Context
Here are some recent changes that were made to this file to help you understand the development context:

{recent_changes}

These recent changes show the development progression leading up to the current task.

The new feature is {query}.

And here is the code snippet you are asked to modify:
```java
{original_data['prompt'].split('And here is the code snippet you are asked to modify:')[-1].split('```java')[-1].split('```')[0].strip()}
```

Please analyze the mission carefully and thoroughly first, and then give a definitely runnable code. You should put your code between ```java and ```."""

    return prompt

def main():
    """ä¸»å‡½æ•°"""
    print("å¼€å§‹é‡æ–°æ„é€ F20-40 benchmark...")
    
    # åŠ è½½åŸå§‹æ•°æ®
    original_data = load_original_benchmark_data()
    
    # å¤„ç†æ‰€æœ‰GPT-5ç»“æœæ–‡ä»¶
    gpt5_dir = 'gpt5_results_20-40'
    benchmark_entries = []
    
    for filename in sorted(os.listdir(gpt5_dir)):
        if filename.endswith('.txt'):
            file_path = os.path.join(gpt5_dir, filename)
            file_id = filename.replace('.txt', '')
            
            print(f"å¤„ç†æ–‡ä»¶: {filename}")
            
            if file_id not in original_data:
                print(f"è­¦å‘Š: æœªæ‰¾åˆ° {file_id} çš„åŸå§‹æ•°æ®")
                continue
            
            try:
                # è§£æGPT-5ç»“æœ
                gpt5_data = parse_gpt5_file(file_path)
                
                # åˆ›å»ºå®Œæ•´prompt
                new_prompt = create_complete_prompt(original_data[file_id], gpt5_data['hunks'])
                
                # åˆ›å»ºæ–°çš„benchmarkæ¡ç›®
                new_entry = original_data[file_id].copy()
                new_entry['prompt'] = new_prompt
                
                benchmark_entries.append(new_entry)
                print(f"âœ… {file_id} å¤„ç†æˆåŠŸ")
                
            except Exception as e:
                print(f"âŒ å¤„ç† {filename} æ—¶å‡ºé”™: {e}")
                continue
    
    # ä¿å­˜ç»“æœ
    output_file = 'benchmark/nl2code_java_F20-40_with_rc_separated_final.jsonl'
    with open(output_file, 'w', encoding='utf-8') as f:
        for entry in benchmark_entries:
            f.write(json.dumps(entry, ensure_ascii=False) + '\n')
    
    print(f"\nğŸ‰ è½¬æ¢å®Œæˆï¼")
    print(f"æˆåŠŸå¤„ç†: {len(benchmark_entries)}/20 æ¡æ•°æ®")
    print(f"è¾“å‡ºæ–‡ä»¶: {output_file}")

if __name__ == "__main__":
    main()
